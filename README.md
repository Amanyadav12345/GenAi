# ğŸ” Multimodal Search with CLIP
A Python project that enables semantic search across both text and images using OpenAI's CLIP model. Users can input a natural language query and retrieve the most relevant **text chunks** and **images** from a set of documents.

## ğŸš€ Features
- âœ… Extracts text and images from PDF documents
- âœ… Generates embeddings using [CLIP (ViT-L/14)](https://huggingface.co/openai/clip-vit-large-patch14)
- âœ… Performs cosine similarity-based search over both modalities
- âœ… Streamlit UI for live querying and filtering
- âœ… Extendable with OCR, FAISS, captions, filters, etc.

## ğŸ—‚ï¸ Project Structure
See folder layout inside this repo.

## ğŸ“¦ Setup Instructions
```bash
pip install -r requirements.txt
```

## âš™ï¸ Pipeline Steps
```bash
python scripts/1_chunk_texts.py
python scripts/2_extract_images.py
python scripts/3_generate_embeddings.py
python scripts/4_search.py
```

## ğŸ–¥ï¸ Streamlit UI (Optional)
```bash
streamlit run app/streamlit_app.py
```

## ğŸ“Œ Future Enhancements
- OCR for images
- FAISS for fast search
- Filter by metadata
- Image-to-text search

## ğŸ¤ License
MIT License
